# Big Data

## What is Big Data?

Big data can be defined through three aspects:

1. volume: a large amount of data that increasingly requires more storage space
2. velocity: an amount of data that is growing fast
3. variety: data that is generated in different formats

Data set of big data to explorer: https://www.forbes.com/sites/bernardmarr/2016/02/12/big-data-35-brilliant-and-free-data-sources-for-2016/?sh=72a49a32b54d

## What are the challenges of big data?

The rapid growth of data can be an advantage or an obstacle when it comes to achieving business goals. To be successful, enterprises must be able to easily access and manage their data assets. With this enormous amount of data being constantly created, traditional technologies and data warehouses cannot keep up with storage needs. Even with the cloud storage facilities that are available from companies like Amazon, Google, Microsoft, and many others, the security of stored data becomes a big problem. Big Data solutions must be secure, have a high fault tolerance, and use replication to ensure data does not get lost. Big Data storage is not only about storing data, it is also about managing and securing it.

1. Management: data can be generated and collected from multiple different sources so a management system must be used to organize and collate all of the sources. There are few data-sharing standards and thousands of data management tools available.

2. Security: for data to be of value to enterprise, it must be kept secure and only accessible by authorized users

3. Redundancy: safeguards should be in place to maintain the integrity of the stored data. Processes for backups, redundancy, and disaster recovery are required.

4. Analytics: big data contains both structured and unstructured data. Unstructured data, such as emails or photos, are difficult to analyze and consume a lot of storage space. Structured data, such as a spreadsheet, is easier to analyze and store. Data storage systems must be able to manage both types.

5. Access: bit data must be accessible from anywhere at anytime. Storage solutions need to support the quantity of input and output requests. Companies should also be aware of the strain placed on WAN links.

## Where can we store big data?

Big data is typically stored on multiple servers, usually housed within data centers. For security, accessibility, and redundancy, the data is usually distributed and/or replicated on many different servers in many different data centers.

### Fog Computing

**Fog computing** is an architecture that utilizes **end-user clients or “edge” devices to do a substantial amount of the pre-processing and storage required by an organization**. Fog computing was designed to keep the data closer to the source for pre-processing.

Sensor data, in particular, can be pre-processed closer to where it was collected. The information gained from that pre-processed analysis can be fed back into the companies’ systems to modify processes if required. Because the sensor data is pre-processed by end devices within the company system, communications to and from the servers and devices would be quicker. This requires less bandwidth than constantly going out to the cloud.

After the data has been pre-processed, it is often shipped off for longer term storage, backup, or deeper analysis within the cloud.

### Cloud and Cloud Computing

The cloud is a collection of data centers or groups of connected servers. Access to software, storage, and services available on the servers is obtained through the Internet via a browser interface. Cloud services are provided by many large companies such as Google, Microsoft, and Apple. Cloud storage services are provided by different vendors such as: Google Drive, Apple iCloud, Microsoft OneDrive, and Dropbox.

From an individual’s perspective, using the cloud services allows you:

* To store all of your data, such as pictures, music, movies, and emails, freeing up local hard drive space
* To access many applications instead of downloading them onto your local device
* To access your data and applications anywhere, anytime, and on any device
* One of the disadvantages of using the cloud is that your data could fall into the wrong hands. Your data is at the mercy of the security robustness of your chosen cloud provider.

From the perspective of an enterprise, cloud services and computing support a variety of data management issues:

* It enables access to organizational data anywhere and at any time.
* It streamlines the IT operations of an organization by subscribing only to needed services.
* It eliminates or reduces the need for onsite IT equipment, maintenance, and management.
* It reduces the cost of equipment, energy, physical plant requirements, and personnel training needs.
* It enables rapid responses to increasing data volume requirements.

## Distributed Processing

With the explosion of business automation systems and the exponential growth of web applications and machine-generated data, analytics is becoming increasingly more difficult to manage. **In fact, 90% of data that exists today has been generated in just the last two years**. This increased volume within a short period of time is a property of exponential growth. This high volume of data is difficult to process and analyze within a reasonable amount of time.

**Rather than large databases being processed by big and powerful mainframe computers and stored in giant disk arrays (vertical scaling), distributed data processing takes the large volume of data and breaks it into smaller pieces**. These smaller data volumes are distributed in many locations to be processed by many computers with smaller processors. Each computer in the distributed architecture analyzes its part of the Big Data picture (horizontal scaling).

Most distributed file systems are designed to be invisible to client programs. The distributed file system locates files and moves data, but the users have no way of knowing that the files are distributed among many different servers or nodes. The users access these files as if they were local to their own computers. All users see the same view of the file system and are able to access data concurrently with other users.

**Hadoop** was created to deal with these Big Data volumes. The Hadoop project started with two facets: The Hadoop Distributed File System (HDFS) is a distributed, fault tolerant file system, and MapReduce, which is a distributed way to process data. Hadoop has now evolved into a very comprehensive ecosystem of software for Big Data management. **Hadoop is open-source software enabling the distributed processing of large data sets that can be terabytes in size and that are stored in clusters of computers**. Hadoop is designed to scale up from single servers to thousands of machines, each offering local computation and storage. To make it more efficient, Hadoop can be installed and run on many VMs. These VMs can all work together in parallel to process and store the data.

Hadoop has two main features that have made it the industry standard for handling Big Data:

* Scalability: Larger cluster sizes improve performance and provide higher data processing capabilities. With Hadoop, cluster size can easily scale from a five node cluster to a one thousand node cluster without excessively increasing the administrative burden.
* Fault tolerance: Hadoop automatically replicates data across clusters to ensure data will not be lost. If a disk, node, or a whole rack fails, the data is safe.

## Analyzing Big Data for Effective Use in Business

Data analysis is the process of inspecting, cleaning, transforming, and modeling data to uncover useful information. Analyzing big data typically requires tools and applications created for this purpose. These analysis tools have been designed to provide businesses with detailed information, patterns, and valuable insights.

Before beginning any analysis, it is critical to know what problem the business is trying to solve or what information the business is looking for.

